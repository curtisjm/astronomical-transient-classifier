{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.youtube.com/watch?v=JoR5HCs0n0s\n",
    "##\n",
    "## Using a simple autoencoder architcture to begin with on the BAT157 mo dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Big array file\n",
    "dir='/Users/acucchiara/COM/proposal/SRP2021/work/'\n",
    "\n",
    "##\n",
    "b157file='BAT150_sim10_wl6_bl_5.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reade the array\n",
    "bat157=np.load(dir+b157file)\n",
    "bat157.files\n",
    "images, labels=bat157['image'],bat157['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1297691960452712\n"
     ]
    }
   ],
   "source": [
    "print((images[:]/0.0487).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = keras.Input(shape=(8, 155, 1), name='bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening the data and creating the bottleneck\n",
    "x = keras.layers.Flatten()(encoder_input)\n",
    "encoder_output = keras.layers.Dense(20, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.Model(encoder_input, encoder_output, name='encoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = keras.layers.Dense(20, activation=\"relu\")(encoder_output)\n",
    "x = keras.layers.Dense(1240, activation=\"relu\")(decoder_input)  # 1240 is 8*155 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "155*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = keras.layers.Reshape((8, 155, 1))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bat (InputLayer)             [(None, 8, 155, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1240)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                24820     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1240)              26040     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 8, 155, 1)         0         \n",
      "=================================================================\n",
      "Total params: 51,280\n",
      "Trainable params: 51,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = keras.Model(encoder_input, decoder_output, name='autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll now compile our model with the optimizer and a loss metric. \n",
    "#We'll use mean squared error for loss (mse).\n",
    "# https://keras.io/api/losses/   for more loss functions\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "autoencoder.compile(opt, loss='mse',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 4.8478e-05 - val_loss: 3.2825e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3.7770e-05 - val_loss: 3.2449e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3.2573e-05 - val_loss: 3.1738e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3.3045e-05 - val_loss: 3.2491e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-1.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3.0112e-05 - val_loss: 3.2903e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3.1362e-05 - val_loss: 3.2249e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.9399e-05 - val_loss: 3.2496e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3.0370e-05 - val_loss: 3.2765e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-2.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.5029e-05 - val_loss: 3.2186e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.4225e-05 - val_loss: 3.2335e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.4902e-05 - val_loss: 3.2612e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.5054e-05 - val_loss: 3.2258e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-3.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.0733e-05 - val_loss: 3.2140e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.2590e-05 - val_loss: 3.2375e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.0109e-05 - val_loss: 3.3589e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.1478e-05 - val_loss: 3.2513e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-4.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.9561e-05 - val_loss: 3.2341e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.0459e-05 - val_loss: 3.2421e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 2.0299e-05 - val_loss: 3.2430e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.7853e-05 - val_loss: 3.2405e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-5.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.9895e-05 - val_loss: 3.3234e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.6567e-05 - val_loss: 3.2410e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.5734e-05 - val_loss: 3.2455e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.6897e-05 - val_loss: 3.2171e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-6.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.6894e-05 - val_loss: 3.2183e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.6653e-05 - val_loss: 3.2550e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 1.4082e-05 - val_loss: 3.2713e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 1.4395e-05 - val_loss: 3.2401e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-7.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.6901e-05 - val_loss: 3.2403e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.3133e-05 - val_loss: 3.2260e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.2752e-05 - val_loss: 3.2382e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.2555e-05 - val_loss: 3.2240e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-8.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.4810e-05 - val_loss: 3.2501e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.3360e-05 - val_loss: 3.2303e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.2546e-05 - val_loss: 3.2612e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.2568e-05 - val_loss: 3.2125e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-9.model/assets\n",
      "Epoch 1/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.2514e-05 - val_loss: 3.3207e-07\n",
      "Epoch 2/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.3327e-05 - val_loss: 3.2341e-07\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1.3270e-05 - val_loss: 3.1751e-07\n",
      "Epoch 4/4\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 1.1833e-05 - val_loss: 3.3021e-07\n",
      "INFO:tensorflow:Assets written to: models/AE-10.model/assets\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    history = autoencoder.fit(\n",
    "      images/0.0487,\n",
    "      images/0.0487,\n",
    "      epochs=4, \n",
    "      batch_size=32, validation_split=0.10\n",
    "        )   \n",
    "    autoencoder.save(f\"models/AE-{epoch+1}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "# First, let's look at an encoded example, because it's cool:\n",
    "example = encoder.predict([images[0].reshape(-1, 8, 155, 1)] )\n",
    "print(example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[0.03825222 0.         0.05131484 0.         0.         0.\n",
      " 0.         0.         0.04366006 0.01554061 0.         0.\n",
      " 0.         0.06293763 0.02478426 0.         0.04863582 0.\n",
      " 0.         0.         0.37470555 0.03252885 0.         0.\n",
      " 0.         0.         0.03096338 0.         0.         0.\n",
      " 0.         0.         0.04247659 0.         0.03688449 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00914979]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1626eb950>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(example[0].shape)\n",
    "print(example[0])\n",
    "plt.imshow(example[0].reshape((2,25)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16216c750>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#That object was:\n",
    "plt.imshow(images[0]*0.0487, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AFTER THE AUTOENCODER IT WAS:\n",
    "ae_out = autoencoder.predict([ images[1].reshape(-1, 8, 155, 1) ])\n",
    "img = ae_out[0]  # predict is done on a vector, and returns a vector, even if its just 1 element, so we still need to grab the 0th\n",
    "plt.imshow(ae_out[0]/0.0487, cmap=\"gray\")\n",
    "print(ae_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using OpenCV, we can quickly cycle through a bunch of \n",
    "#examples by doing:\n",
    "\n",
    "for d in images[:5]:  # just show 5 examples, feel free to show all or however many you want!\n",
    "\n",
    "    ae_out = autoencoder.predict([ d.reshape(-1, 8, 155, 1) ])\n",
    "    img = ae_out[0]\n",
    "\n",
    "    cv2.imshow(\"decoded\",img )\n",
    "    cv2.imshow(\"original\",np.array(d))\n",
    "    cv2.waitKey(1000)  # wait 1000ms, 1 second, and then show the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following the suggestion of Peter Harrington:\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "#https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the predicted images from the encoder portion\n",
    "X=[]\n",
    "\n",
    "for im in images:\n",
    "    val=encoder.predict(im.reshape(-1, 8, 155, 1) )\n",
    "    #print(val.shape)\n",
    "    X.append(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.07632875 0.         0.         0.\n",
      "  0.         0.         0.07497109 0.15977375 0.         0.\n",
      "  0.         0.21994528 0.         0.         0.08159518 0.\n",
      "  0.         0.         0.35699314 0.04200723 0.         0.\n",
      "  0.         0.         0.00399086 0.         0.         0.\n",
      "  0.         0.05970611 0.01959756 0.         0.02944804 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.0761209  0.         0.         0.\n",
      "  0.         0.         0.07511173 0.16012155 0.         0.\n",
      "  0.         0.21992758 0.         0.         0.08120699 0.\n",
      "  0.         0.         0.3569238  0.04217125 0.         0.\n",
      "  0.         0.         0.00393685 0.         0.         0.\n",
      "  0.         0.05955221 0.01926135 0.         0.02957396 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(X[0:2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10822, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), labels, test_size=0.3)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=30)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[ 1  1  1  2 20  1  1  1 25]\n"
     ]
    }
   ],
   "source": [
    "print(neigh.predict([X_test[3]]))\n",
    "print(y_test[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
